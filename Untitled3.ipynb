{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPW/NVAlgRaVTxeRnv6q1QE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunRise9551/Building_Makemore/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review"
      ],
      "metadata": {
        "id": "_l4Ydpc5N7s4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pBRXch5hK8QF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "XrvO2Lv6OJmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "id": "8hH0EEsnK91p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b581f766-1ea9-468b-f7e1-2bb397fbce61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-17 08:09:47--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-09-17 08:09:47 (15.0 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "words[:8]"
      ],
      "metadata": {
        "id": "ZvoPjnAwK_IJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ddf807-f1e7-4eff-ed55-1eb5f474f66d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "FDOJS3P_LACx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c961b60c-f902-4147-dd13-a4d4260c8aea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32033"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "2cVSHeUHLBCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd71761-bb17-4609-9910-45de5e6ed153"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
      ],
      "metadata": {
        "id": "1Z329h1JLC8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6883e8e-91f3-464c-df1e-3ad792714eff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP Revisited"
      ],
      "metadata": {
        "id": "j1Np19axOFCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP revisited\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
        "b1 = torch.randn(n_hidden,                        generator=g)\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g)\n",
        "b2 = torch.randn(vocab_size,                      generator=g)\n",
        "\n",
        "parameters = [C, W1, W2, b2]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "0CoRg_BVLE0Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dc6e7a-8b8a-47c9-af12-3b127139197e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixing Initial Loss"
      ],
      "metadata": {
        "id": "tyoM9pRCOuPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Issue"
      ],
      "metadata": {
        "id": "EJw5N39TP9Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code is Optimization step, we want to identify the Initial Loss"
      ],
      "metadata": {
        "id": "u0OSJHzoOy3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAAKbsb_L-aJ",
        "outputId": "31172ee7-a462-4e96-cd61-8f12cc8eea52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 28.4938\n",
            "  10000/ 200000: 2.6654\n",
            "  20000/ 200000: 2.6381\n",
            "  30000/ 200000: 2.7673\n",
            "  40000/ 200000: 2.1780\n",
            "  50000/ 200000: 2.6614\n",
            "  60000/ 200000: 2.2672\n",
            "  70000/ 200000: 1.9174\n",
            "  80000/ 200000: 2.2796\n",
            "  90000/ 200000: 2.3978\n",
            " 100000/ 200000: 1.9056\n",
            " 110000/ 200000: 2.3627\n",
            " 120000/ 200000: 1.9990\n",
            " 130000/ 200000: 2.4281\n",
            " 140000/ 200000: 2.3419\n",
            " 150000/ 200000: 2.1175\n",
            " 160000/ 200000: 1.9509\n",
            " 170000/ 200000: 1.8603\n",
            " 180000/ 200000: 2.0507\n",
            " 190000/ 200000: 2.0213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Problem: Our initial Loss is way too high.\n",
        "- Solution: Make probability distribution Uniform.\n"
      ],
      "metadata": {
        "id": "k3RaKzICPS0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example of Issues"
      ],
      "metadata": {
        "id": "ISrTR-Y_P5Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1"
      ],
      "metadata": {
        "id": "ihl4SQieQQuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([0.0, 0.0, 0.0, 0.0])\n",
        "probs = torch.softmax(logits, dim=0)\n",
        "loss = -probs[2].log()\n",
        "probs, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNeOPWrDOqN_",
        "outputId": "054ba350-709c-4286-ad27-8fabf1a5430a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.2500, 0.2500, 0.2500, 0.2500]), tensor(1.3863))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2"
      ],
      "metadata": {
        "id": "tkfAsJHSQR5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([0.0, 2.0, 5.0, 1.0])\n",
        "probs = torch.softmax(logits, dim=0)\n",
        "loss = -probs[2].log()\n",
        "probs, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQdKgNtgP2uW",
        "outputId": "05b4110d-b903-4abd-ae60-6cc89595ae71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0063, 0.0463, 0.9304, 0.0170]), tensor(0.0722))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 3"
      ],
      "metadata": {
        "id": "5kEzDc3gQPqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([0.0, 2.0, 5.0, 1.0])\n",
        "probs = torch.softmax(logits, dim=0)\n",
        "loss = -probs[0].log()\n",
        "probs, loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYQ5mN3MQPdg",
        "outputId": "f8ac4a09-6be5-4dde-fdca-fecb0d6fa59d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.0063, 0.0463, 0.9304, 0.0170]), tensor(5.0722))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1: Loss = 1.386\n",
        "- Probability distribution is uniform\n",
        "\n",
        "Example 2: Loss = 0.072\n",
        "- Loss is low, because we predicted the correct output\n",
        "- probs[2], 3rd index in logits is the highest\n",
        "\n",
        "Example 3: Loss = 5.077\n",
        "- Loss is high, because we predicted the wrong output\n",
        "- probs[0], 1st index in logits is not the highest"
      ],
      "metadata": {
        "id": "6KasSSQRQZAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we want:\n",
        "- Uniform distribution of probability\n",
        "- We can actually make the probability either:\n",
        "    - All Zeros\n",
        "    - All Same Values"
      ],
      "metadata": {
        "id": "fsR5j3uGRyxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identify Issue in Logits"
      ],
      "metadata": {
        "id": "xoEFkjnCSbqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Break on first loop\n",
        "- Identify our logits"
      ],
      "metadata": {
        "id": "4WzykwDPSm0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCWHuiMwRDcU",
        "outputId": "d8e12e62-6e0b-4f29-fe41-666e909e3625"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 2.1791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking only the first row of logits"
      ],
      "metadata": {
        "id": "4Ve2IVCLS0Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logits\n",
        "logits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuxqUpo_SxMW",
        "outputId": "5dd7505c-4414-42c1-9b48-5b74ad3f8ef7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-4.6722,  0.3866, -1.0412, -1.5210,  0.7820, -1.1636, -1.6314, -2.2349,\n",
              "        -1.3722,  0.1318, -1.8610, -1.5670,  2.4515,  1.9424,  0.9991, -0.6578,\n",
              "        -1.9764, -5.5571,  1.0462,  1.4402, -0.1736, -0.5661,  1.6520, -2.4421,\n",
              "        -2.1513, -0.3296,  0.2990], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our logits is taking extreme values"
      ],
      "metadata": {
        "id": "e_0ksX7nS6XH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "-tCD9-5QTyOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-Initialization Network"
      ],
      "metadata": {
        "id": "kT-NDa6pT0Rn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Our logits is --> logits = h @ W2 + b2\n",
        "2. We don't want bias, so we make b2 * 0\n",
        "3. We want our W2 to be small, so the output loss will be low"
      ],
      "metadata": {
        "id": "pE7OCa3sUSue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP revisited\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
        "b1 = torch.randn(n_hidden,                        generator=g)\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01 # <---------- modify W2\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0    # <---------- modify b2\n",
        "\n",
        "parameters = [C, W1, W2, b2]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eASLjwkzSzQ8",
        "outputId": "b8ac4213-1be6-4d52-93bb-d333f7e63094"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-Optimization"
      ],
      "metadata": {
        "id": "rJzbMKWGV6Yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Break after first loop, and check logits"
      ],
      "metadata": {
        "id": "2UrCHtwGWBfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilors9FOUPo_",
        "outputId": "842b0b56-2c58-4e04-df42-a578cab9d2f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.3277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logits are now much closer to 0"
      ],
      "metadata": {
        "id": "-nAAT4HKVl6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juun8GVZUQbf",
        "outputId": "c3f87aca-26c4-4c74-e838-e76b4fb62080"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0460,  0.3116, -0.1364,  0.0526,  0.1807, -0.0630, -0.0736, -0.0335,\n",
              "        -0.0208,  0.1213,  0.1839, -0.0565,  0.0649, -0.2445, -0.1240,  0.0556,\n",
              "         0.0023, -0.1085, -0.2924, -0.0622,  0.2324,  0.1036,  0.1477, -0.1076,\n",
              "        -0.2047,  0.0865,  0.1166], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-run full Optimization"
      ],
      "metadata": {
        "id": "R8WTBM-yWKPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "lossi = []\n",
        "\n",
        "for i in range(max_steps):\n",
        "\n",
        "  # minibatch construct\n",
        "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "  # forward pass\n",
        "  emb = C[Xb] # embed the characters into vectors\n",
        "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "  # Linear layer\n",
        "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
        "  # Non-linearity\n",
        "  h = torch.tanh(hpreact) # hidden layer\n",
        "  logits = h @ W2 + b2 # output layer\n",
        "  loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "  # backward pass\n",
        "  for p in parameters:\n",
        "    p.grad = None\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "  for p in parameters:\n",
        "    p.data += -lr * p.grad\n",
        "\n",
        "  # track stats\n",
        "  if i % 10000 == 0: # print every once in a while\n",
        "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "  lossi.append(loss.log10().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh24rweYVnIZ",
        "outputId": "fced9684-c952-4990-c86e-f8b87a43fcbe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0/ 200000: 3.2144\n",
            "  10000/ 200000: 1.9288\n",
            "  20000/ 200000: 1.9662\n",
            "  30000/ 200000: 2.0635\n",
            "  40000/ 200000: 2.0994\n",
            "  50000/ 200000: 2.2196\n",
            "  60000/ 200000: 2.1243\n",
            "  70000/ 200000: 2.7595\n",
            "  80000/ 200000: 2.5463\n",
            "  90000/ 200000: 2.2940\n",
            " 100000/ 200000: 2.4423\n",
            " 110000/ 200000: 1.8244\n",
            " 120000/ 200000: 2.2097\n",
            " 130000/ 200000: 1.9917\n",
            " 140000/ 200000: 1.8389\n",
            " 150000/ 200000: 2.3497\n",
            " 160000/ 200000: 2.2176\n",
            " 170000/ 200000: 1.8031\n",
            " 180000/ 200000: 1.9221\n",
            " 190000/ 200000: 2.1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lossi)"
      ],
      "metadata": {
        "id": "7h6y3hU2WO_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}